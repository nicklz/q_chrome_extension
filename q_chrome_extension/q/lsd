#!/bin/bash
# =====================================================================================
# ğŸ¤– FILE DUMP â€” SUPER MODE (Legacy "lsd a" full-dump vs. Filtered "lsd a <filters>")
# =====================================================================================
# Fork rules (as required):
#   â€¢ "lsd a"           => LEGACY MODE. Full tree dump + full/large cat prints + analytics.
#                          This path NEVER shares selection/mismatch logic with filter mode.
#   â€¢ "lsd a <filters>" => FILTER MODE. Exact path/basename matching, planned vs actual,
#                          command verification (NO /tmp â€¦ artifacts), related files,
#                          word clouds, n-grams, biggest files, longest/rarest token.
#
# Parsing rules for <filters> (in order):
#   1) If the raw filter string looks like JSON array (starts '[' and ends ']'): parse JSON.
#   2) Else split by commas, trim each token.
#   3) Heuristic for unquoted CLI: if the second space-separated token looks like â€œname.extâ€
#      (exactly one dot, both sides alnum/_/-), count it as a valid filename token.
# Planned count:
#   â€¢ Increments for each token that â€œlooks like a filenameâ€ (allows subpaths) â€” independent of existence.
#   â€¢ Also increments for tokens that match any exact path or exact basename in the tree (normalized equality).
#   â€¢ Never reduced by existence; e.g., "requirements.txt, test2.txt" => planned = 2.
#
# Directory tree:
#   â€¢ Collapses node_modules/, vendor/, .terraform/, template*/ into one summary line each:
#     â€œ[# files; TOTAL_KB] Top3: <largest three basenames>â€
#
# Printing:
#   â€¢ Text files: full content unless >300KB (then first 300KB) but still in analytics.
#   â€¢ JSON files: DO NOT print file contents; print Tag Cloud only (still included in analytics).
#   â€¢ Binary-like extensions are listed but never catâ€™d.
#
# Final status line:
#   â€¢ âœ… only if actual == planned (e.g., â€œâœ… All file dumps complete. [N files total | N files guessed]â€)
#   â€¢ ğŸŸ¡ otherwise, â€œğŸŸ¡ All file dumps complete. [<actual> files total | <planned> files guessed]â€
#
# Command Verification block:
#   â€¢ COMMAND line shows a clean, human version:
#       - LEGACY:   â€œlsd aâ€
#       - FILTER:   â€œlsd a requirements.txt, README.mdâ€
#     (No /tmp paths, no internal function args, no â€œSELECTED filterâ€.)
#   â€¢ Also shows â€œPlanned files: <number>â€ and â€œ[Q] Version 5.2â€
#
# -------------------------------------------------------------------------------------
# PATCH NOTES (applies ONLY when scope starts with: lsd a ...)
#   â€¢ Output chunking in legacy/filter loops (default 3 files per chunk + 1s pause)
#   â€¢ Aggressive pruning of dependency/build/cache dirs across many languages/package managers
#     (node_modules, vendor, target, dist, build, etc.) to prevent huge repo hangs.
#   â€¢ Performance optimizations: prune (no traversal), cached file sizes, fewer external calls,
#     faster text detection, capped per-file tag clouds.
# -------------------------------------------------------------------------------------
# =====================================================================================
clear
set -u

# ---------------------------------------------------------------------
# macOS compatibility: require Bash >= 4 (associative arrays, namerefs, mapfile)
# On macOS, /bin/bash is usually 3.2 and will break this script.
# Auto-reexec into Homebrew bash if available.
# ---------------------------------------------------------------------
if [[ -z "${BASH_VERSINFO:-}" || "${BASH_VERSINFO[0]}" -lt 4 ]]; then
  # Prefer Apple Silicon Homebrew path, then Intel Homebrew path
  for B in /opt/homebrew/bin/bash /usr/local/bin/bash; do
    if [[ -x "$B" ]]; then
      exec "$B" "$0" "$@"
    fi
  done

  echo "error: Bash >= 4 is required (associative arrays, nameref, mapfile)."
  echo "error: macOS default /bin/bash is usually 3.2."
  echo "fix:"
  echo "  brew install bash"
  echo "  then re-run: $0 $*"
  exit 2
fi


# Capture a clean display of user's invocation for later (sanitized)
SCRIPT_BASENAME="$(basename "$0")"
RAW_ARGS="$*"

# Only apply chunking + aggressive pruning + extra optimization when the scope is "a"
A_MODE=false
if [[ "$#" -gt 0 && "$1" == "a" ]]; then
  A_MODE=true
fi

# Chunking controls (ONLY used in 'a' modes)
CHUNK_SIZE="${LSD_CHUNK_SIZE:-3}"
CHUNK_SLEEP="${LSD_CHUNK_SLEEP:-1}"

echo -e "\n\nğŸš€âœ¨ğŸ¤– FILE DUMP â€” SUPER MODE ğŸ¤–âœ¨ğŸš€"
echo -e "ğŸ“‚ Scan Target: \033[1m.\033[0m â± $(date '+%Y-%m-%d %H:%M:%S') ğŸ§  Analytics: ON"

# ---------------------------------------------------------------------
# Platform + constants
# ---------------------------------------------------------------------
SCAN_DIR="."
OS_TYPE="$(uname)"
IS_MAC=false
IS_LINUX=false
[[ "$OS_TYPE" == "Darwin" ]] && IS_MAC=true
[[ "$OS_TYPE" == "Linux"  ]] && IS_LINUX=true

# Exclusions (keep *.json INCLUDED)
# NOTE: These are the ORIGINAL patterns (preserved for non-'a' scopes).
EXCLUDE_PATTERNS=(
  "*/.git/*" "*/__pycache__/*" "*/env/*" "*/venv/*"
  "*/*/env/*" "**/data/*" "*.png" "*.jpg"
  "*/images/*" "*/files/*" "*/build/*" "*/.build/*"
  "*/*/venv/*" "*/*env*/**" "*/.env/*" "*.build*"
  "*/tts_env/*" "*/venv/*"
  "*/v5/api/modules/contrib/*"
  "./.cache/*"
  "./node_modules/*"
  "package-lock.json"
  "./vendor/*"
  "*.zip" "*.gz" "*.gzip" "*.glb"
)

# Binary-like extensions (never cat)
BINARY_EXTENSIONS="jpg|jpeg|png|gif|bmp|ico|webp|pdf|mp3|mp4|mov|zip|tar|gz|gzip|bz2|xz|7z|exe|dll|so|dylib|bin|ttf|otf|woff|woff2|glb"

# Convert exclusions to find switches (legacy/non-'a' scopes)
EXCLUDES=()
for PATTERN in "${EXCLUDE_PATTERNS[@]}"; do
  EXCLUDES+=( -not -path "$PATTERN" )
done

# ---------------------------------------------------------------------
# Ultra-prune dirs (ONLY used in 'a' modes) to avoid massive dependency trees
# ---------------------------------------------------------------------
# We prune by *directory name* so it works anywhere in the tree (not just ./node_modules).
PRUNE_DIR_NAMES=(
  # VCS
  ".git" ".hg" ".svn" ".bzr"

  # Node / JS package managers + tooling caches
  "node_modules" "bower_components" "jspm_packages"
  ".npm" ".yarn" ".pnpm-store" ".parcel-cache" ".turbo" ".vite" ".next" ".nuxt" ".svelte-kit" ".astro" ".vercel"

  # Python
  "__pycache__" ".pytest_cache" ".mypy_cache" ".ruff_cache" ".tox" "venv" ".venv" "env" ".eggs" ".hypothesis"

  # PHP / Go / general vendoring
  "vendor"

  # Terraform
  ".terraform"

  # Java / Gradle / Maven / Kotlin / Scala
  "target" ".gradle"

  # .NET
  "bin" "obj"

  # Builds / distributions (common across languages)
  "build" ".build" "dist" "out" ".out" "coverage" ".nyc_output"

  # iOS / macOS
  "Pods" "Carthage" "DerivedData" ".swiftpm"

  # Dart / Flutter
  ".dart_tool" ".pub-cache"

  # Elixir / Erlang
  "_build" "deps"

  # Haskell
  ".stack-work" "dist-newstyle"

  # CMake
  "CMakeFiles"

  # IDE/editor
  ".idea" ".vscode"

  # Misc caches
  ".cache"
)

# Some prune patterns are best expressed as globs
PRUNE_DIR_GLOBS=(
  "cmake-build-*"
)

# Extra file/path exclusions for 'a' modes (we keep the original list AND add some)
EXCLUDE_PATTERNS_A_MODE=(
  "${EXCLUDE_PATTERNS[@]}"
  # Make root-only patterns apply anywhere (defensive, even though we prune)
  "*/node_modules/*"
  "*/vendor/*"
  "*/.terraform/*"
  "*/.cache/*"
)

EXCLUDES_A_MODE=()
for PATTERN in "${EXCLUDE_PATTERNS_A_MODE[@]}"; do
  EXCLUDES_A_MODE+=( -not -path "$PATTERN" )
done

# ---------------------------------------------------------------------
# Global caches (ONLY populated/used when A_MODE=true)
# ---------------------------------------------------------------------
ALL_FILES_BUILT=false
declare -a ALL_FILES
declare -A FILE_SIZE_BYTES
declare -A FILE_SIZE_KB
declare -A PATH_NORM_SET
declare -A BASENAME_SET
declare -A BASENAME_TO_FILES
declare -A NORMPATH_TO_FILE

# Feature detection (for optimized scans)
FIND_HAS_PRINTF=false
if find "$SCAN_DIR" -maxdepth 0 -printf "" >/dev/null 2>&1; then
  FIND_HAS_PRINTF=true
fi

SORT_HAS_Z=false
if printf "" | sort -z >/dev/null 2>&1; then
  SORT_HAS_Z=true
fi

have_cmd() { command -v "$1" >/dev/null 2>&1; }

# Normalize leading "./" away for consistent comparisons
strip_leading_dotslash() {
  local v="$1"
  [[ "$v" == ./* ]] && echo "${v:2}" || echo "$v"
}

# Build find args for pruning (only used in 'a' modes)
build_prune_match_args() {
  local -n _out="$1"
  _out=()
  local first=true

  for dn in "${PRUNE_DIR_NAMES[@]}"; do
    if $first; then
      _out+=( -name "$dn" )
      first=false
    else
      _out+=( -o -name "$dn" )
    fi
  done

  for dg in "${PRUNE_DIR_GLOBS[@]}"; do
    if $first; then
      _out+=( -name "$dg" )
      first=false
    else
      _out+=( -o -name "$dg" )
    fi
  done
}

# Build (and cache) ALL_FILES + sizes + lookup maps (ONLY for 'a' modes)
ensure_all_files_built() {
  $A_MODE || return 0
  $ALL_FILES_BUILT && return 0

  ALL_FILES=()
  FILE_SIZE_BYTES=()
  FILE_SIZE_KB=()
  PATH_NORM_SET=()
  BASENAME_SET=()
  BASENAME_TO_FILES=()
  NORMPATH_TO_FILE=()

  local -a PRUNE_MATCH=()
  build_prune_match_args PRUNE_MATCH

  if $FIND_HAS_PRINTF; then
    local -a FIND_CMD=(find "$SCAN_DIR"
      "(" -type d "(" "${PRUNE_MATCH[@]}" ")" -prune ")"
      -o -type f "${EXCLUDES_A_MODE[@]}"
      -printf "%p\t%s\0"
    )

    if $SORT_HAS_Z; then
      while IFS=$'\t' read -r -d '' p b; do
        [[ -f "$p" ]] || continue
        ALL_FILES+=("$p")
        FILE_SIZE_BYTES["$p"]="$b"
        FILE_SIZE_KB["$p"]="$(( (b + 1023) / 1024 ))"

        norm="$(strip_leading_dotslash "$p")"
        base="${p##*/}"
        PATH_NORM_SET["$norm"]=1
        BASENAME_SET["$base"]=1
        BASENAME_TO_FILES["$base"]+="$p"$'\n'
        NORMPATH_TO_FILE["$norm"]="$p"
      done < <( "${FIND_CMD[@]}" | LC_ALL=C sort -z )
    else
      while IFS=$'\t' read -r -d '' p b; do
        [[ -f "$p" ]] || continue
        ALL_FILES+=("$p")
        FILE_SIZE_BYTES["$p"]="$b"
        FILE_SIZE_KB["$p"]="$(( (b + 1023) / 1024 ))"

        norm="$(strip_leading_dotslash "$p")"
        base="${p##*/}"
        PATH_NORM_SET["$norm"]=1
        BASENAME_SET["$base"]=1
        BASENAME_TO_FILES["$base"]+="$p"$'\n'
        NORMPATH_TO_FILE["$norm"]="$p"
      done < <( "${FIND_CMD[@]}" )
    fi
  else
    # Fallback: no -printf; do a prune + print0, then stat each file
    local -a FIND_CMD=(find "$SCAN_DIR"
      "(" -type d "(" "${PRUNE_MATCH[@]}" ")" -prune ")"
      -o -type f "${EXCLUDES_A_MODE[@]}"
      -print0
    )

    local -a files=()
    if $SORT_HAS_Z; then
      # sort -z can sort NUL-terminated records
      while IFS= read -r -d '' f; do
        files+=("$f")
      done < <( "${FIND_CMD[@]}" | LC_ALL=C sort -z )
    else
      while IFS= read -r -d '' f; do
        files+=("$f")
      done < <( "${FIND_CMD[@]}" )
    fi

    for p in "${files[@]}"; do
      [[ -f "$p" ]] || continue
      local b=0
      if $IS_LINUX && have_cmd stat; then
        b="$(stat -c %s "$p" 2>/dev/null || echo 0)"
      elif $IS_MAC && have_cmd stat; then
        b="$(stat -f %z "$p" 2>/dev/null || echo 0)"
      else
        b="$(wc -c < "$p" 2>/dev/null | tr -d ' ' || echo 0)"
      fi

      ALL_FILES+=("$p")
      FILE_SIZE_BYTES["$p"]="$b"
      FILE_SIZE_KB["$p"]="$(( (b + 1023) / 1024 ))"

      norm="$(strip_leading_dotslash "$p")"
      base="${p##*/}"
      PATH_NORM_SET["$norm"]=1
      BASENAME_SET["$base"]=1
      BASENAME_TO_FILES["$base"]+="$p"$'\n'
      NORMPATH_TO_FILE["$norm"]="$p"
    done
  fi

  ALL_FILES_BUILT=true
}

# ---------------------------------------------------------------------
# Collapsed directory tree (always first)
# ---------------------------------------------------------------------
print_directory_tree_collapsed() {
  echo -e "\nğŸ“¦ Directory Tree with File Sizes (in KB):\n"

  # For non-'a' scopes: keep the ORIGINAL implementation (identical behavior).
  if ! $A_MODE; then
    declare -A BUCKET_COUNT
    declare -A BUCKET_SIZE
    declare -A BUCKET_TOP3 # "size|path" lines

    detect_bucket() {
      local p="$1"
      local IFS='/'
      read -ra parts <<< "${p#./}"
      local cur="./"
      for seg in "${parts[@]}"; do
        cur="${cur%/}/$seg"
        if [[ "$seg" == "node_modules" || "$seg" == "vendor" || "$seg" == ".terraform" || "$seg" == template* ]]; then
          echo "$cur/"
          return 0
        fi
      done
      echo ""
    }

    while IFS= read -r -d '' file; do
      [[ -f "$file" ]] || continue
      local_size_kb=$(du -k "$file" | cut -f1)
      bucket="$(detect_bucket "$file")"
      if [[ -n "$bucket" ]]; then
        BUCKET_COUNT["$bucket"]=$(( ${BUCKET_COUNT["$bucket"]:-0} + 1 ))
        BUCKET_SIZE["$bucket"]=$(( ${BUCKET_SIZE["$bucket"]:-0} + local_size_kb ))
        entry="$(printf "%012d|%s" "$local_size_kb" "$file")"
        BUCKET_TOP3["$bucket"]+="${entry}"$'\n'
      else
        printf "%s [%sKB]\n" "$file" "$local_size_kb"
      fi
    done < <(find "$SCAN_DIR" -type f "${EXCLUDES[@]}" -print0 | sort -z)

    for b in "${!BUCKET_COUNT[@]}"; do
      top3="$(printf "%s" "${BUCKET_TOP3["$b"]}" | sed '/^$/d' | sort -r | head -n 3 | cut -d'|' -f2-)"
      t1="$(echo "$top3" | sed -n '1p' | awk -F/ '{print $NF}')"
      t2="$(echo "$top3" | sed -n '2p' | awk -F/ '{print $NF}')"
      t3="$(echo "$top3" | sed -n '3p' | awk -F/ '{print $NF}')"
      printf "%s [#%d files; %sKB] Top3: %s%s%s\n" \
        "$b" "${BUCKET_COUNT["$b"]}" "${BUCKET_SIZE["$b"]}" \
        "${t1:-â€”}" \
        "${t2:+, $t2}" \
        "${t3:+, $t3}"
    done
    return 0
  fi

  # 'a' scopes: optimized + pruned scan (no traversal into giant dependency dirs)
  ensure_all_files_built

  declare -A BUCKET_COUNT
  declare -A BUCKET_SIZE
  declare -A BUCKET_TOP3 # "size|path" lines

  detect_bucket() {
    local p="$1"
    local IFS='/'
    read -ra parts <<< "${p#./}"
    local cur="./"
    for seg in "${parts[@]}"; do
      cur="${cur%/}/$seg"
      if [[ "$seg" == "node_modules" || "$seg" == "vendor" || "$seg" == ".terraform" || "$seg" == template* ]]; then
        echo "$cur/"
        return 0
      fi
    done
    echo ""
  }

  for file in "${ALL_FILES[@]}"; do
    [[ -f "$file" ]] || continue
    local_size_kb="${FILE_SIZE_KB["$file"]:-0}"
    bucket="$(detect_bucket "$file")"
    if [[ -n "$bucket" ]]; then
      BUCKET_COUNT["$bucket"]=$(( ${BUCKET_COUNT["$bucket"]:-0} + 1 ))
      BUCKET_SIZE["$bucket"]=$(( ${BUCKET_SIZE["$bucket"]:-0} + local_size_kb ))
      entry="$(printf "%012d|%s" "$local_size_kb" "$file")"
      BUCKET_TOP3["$bucket"]+="${entry}"$'\n'
    else
      printf "%s [%sKB]\n" "$file" "$local_size_kb"
    fi
  done

  for b in "${!BUCKET_COUNT[@]}"; do
    top3="$(printf "%s" "${BUCKET_TOP3["$b"]}" | sed '/^$/d' | sort -r | head -n 3 | cut -d'|' -f2-)"
    t1="$(echo "$top3" | sed -n '1p' | awk -F/ '{print $NF}')"
    t2="$(echo "$top3" | sed -n '2p' | awk -F/ '{print $NF}')"
    t3="$(echo "$top3" | sed -n '3p' | awk -F/ '{print $NF}')"
    printf "%s [#%d files; %sKB] Top3: %s%s%s\n" \
      "$b" "${BUCKET_COUNT["$b"]}" "${BUCKET_SIZE["$b"]}" \
      "${t1:-â€”}" \
      "${t2:+, $t2}" \
      "${t3:+, $t3}"
  done
}

# ---------------------------------------------------------------------
# Helpers (shared)
# ---------------------------------------------------------------------
MAX_PRINT_BYTES=$((300 * 1024))
TOP_WORDS_COUNT=10
TOP_NGRAMS=3
TOP_RELATED_FILES=3
TOP_RELATED_WORDS=3
TOP_BIGGEST_FILES=3
TOP_BIGGEST_WORDS=3
PER_FILE_WORDS=8

normalize_text() {
  # Faster normalization (fewer processes): lowercase + convert non-token runs to '\n'
  tr '[:upper:]' '[:lower:]' | tr -cs 'a-z0-9_' '\n' | sed '/^$/d'
}

filesize_bytes() {
  local f="$1"
  # Use cached sizes when available (A_MODE)
  if $A_MODE; then
    local v="${FILE_SIZE_BYTES["$f"]:-}"
    if [[ -n "$v" ]]; then
      echo "$v"
      return 0
    fi
  fi

  if $IS_LINUX && have_cmd stat; then
    stat -c %s "$f" 2>/dev/null || wc -c < "$f" | tr -d ' '
  elif $IS_MAC && have_cmd stat; then
    stat -f %z "$f" 2>/dev/null || wc -c < "$f" | tr -d ' '
  else
    wc -c < "$f" | tr -d ' '
  fi
}

filesize_kb() {
  local f="$1"
  if $A_MODE; then
    local v="${FILE_SIZE_KB["$f"]:-}"
    if [[ -n "$v" ]]; then
      echo "$v"
      return 0
    fi
  fi
  local b
  b="$(filesize_bytes "$f" 2>/dev/null || echo 0)"
  echo "$(( (b + 1023) / 1024 ))"
}

fullpath_of() {
  local f="$1"
  if have_cmd realpath; then realpath "$f"
  else
    (cd "$(dirname "$f")" 2>/dev/null && printf "%s/%s\n" "$(pwd)" "$(basename "$f")")
  fi
}

is_binary_by_ext() { [[ "$1" =~ \.($BINARY_EXTENSIONS)$ ]] ; }
is_json_file() { [[ "$1" =~ \.json$ ]]; }

# Fast-ish text detection (avoid calling `file` on every file)
is_text_file_fast() {
  local f="$1"

  # Extension-based fast-path for common text
  local base="${f##*/}"
  local ext="${base##*.}"

  # Treat dotfiles without extension as text in many cases
  case "$base" in
    Makefile|Dockerfile|CMakeLists.txt|Gemfile|Rakefile|Brewfile|Podfile|Pipfile|Procfile) return 0 ;;
    .* ) : ;; # fall through to grep check
  esac

  case "${ext,,}" in
    txt|md|markdown|rst|adoc|log|csv|tsv|ini|cfg|conf|toml|yml|yaml|json|xml|html|htm|css|scss|less|sql|graphql|gql|proto|properties|gradle|env|lock|make|mk|dockerfile|gitignore|gitattributes|editorconfig|sh|bash|zsh|fish|ps1|py|pyi|js|mjs|cjs|ts|tsx|jsx|java|kt|kts|go|rs|c|h|cpp|hpp|cc|cs|php|rb|pl|pm|lua|swift|m|mm|r|jl) return 0 ;;
  esac

  # Heuristic: grep -Iq is typically much faster than `file` and stops early on binary
  LC_ALL=C grep -Iq . "$f" 2>/dev/null
}

word_counts_stream() {
  normalize_text \
  | awk '{f[$0]++} END{for(w in f) print w, f[w]}' \
  | LC_ALL=C sort -k2,2nr -k1,1
}

word_counts_file_capped() {
  local f="$1"
  local cap="$2"
  local b
  b="$(filesize_bytes "$f" 2>/dev/null || echo 0)"
  if [[ "$b" -gt "$cap" ]]; then
    head -c "$cap" "$f" | word_counts_stream
  else
    normalize_text < "$f" | awk '{f[$0]++} END{for(w in f) print w, f[w]}' | LC_ALL=C sort -k2,2nr -k1,1
  fi
}

ngrams_scored() {
  awk '
    {
      n=split(tolower($0), raw, /[^a-z0-9_]+/); m=0;
      for(i=1;i<=n;i++) if(raw[i]!="") a[++m]=raw[i];
      for(i=1;i<=m;i++){
        seq="";
        for(k=1;k<=5 && i+k-1<=m;k++){
          if(seq=="") seq=a[i]; else seq=seq" "a[i+k-1];
          c[seq]++; L[seq]=k;
        }
      }
      delete a
    }
    END{
      for(k in c){
        sc=c[k] + 10*L[k];
        print k, sc, c[k], L[k];
      }
    }' \
  | LC_ALL=C sort -k2,2nr -k3,3nr
}

longest_rarest_token() {
  normalize_text \
  | awk '{f[$0]++} END{max=0;best="";for(w in f){if(f[w]==1 && length(w)>max){max=length(w);best=w}} if(best!="")print best}'
}

# Build all files array (wrapper)
build_all_files_array() {
  if $A_MODE; then
    ensure_all_files_built
    return 0
  fi

  ALL_FILES=()
  while IFS= read -r -d '' file; do
    ALL_FILES+=("$file")
  done < <(find "$SCAN_DIR" -type f "${EXCLUDES[@]}" -print0 | sort -z)
}

all_matches_by_basename() {
  local base="$1"
  if $A_MODE; then
    # Use cached map for speed (preserves ALL_FILES order because we built it in order)
    if [[ -n "${BASENAME_TO_FILES["$base"]:-}" ]]; then
      printf "%s" "${BASENAME_TO_FILES["$base"]}"
    fi
    return 0
  fi

  for f in "${ALL_FILES[@]}"; do
    [[ "${f##*/}" == "$base" ]] && echo "$f"
  done
}

# Filename-like (accept single file or subpath with at least one dot â€” name and ext are [A-Za-z0-9_-])
# Accepts:
#   name.ext
#   ./name.ext
#   path/to/name.ext
#   ./path/to/name.ext
is_clean_filename() {
  local t="$1"
  [[ "$t" =~ ^[.\/]*[A-Za-z0-9_\-\/]+\.([A-Za-z0-9_\-]+)$ ]]
}

# Parse filter tokens from raw string per rules
parse_filter_tokens_from_args() {
  local raw="$*"
  # Rule 1: JSON-looking array?
  if [[ "$raw" =~ ^[[:space:]]*\[.*\][[:space:]]*$ ]]; then
    if command -v python3 >/dev/null 2>&1; then
      python3 - << 'PY' 2>/dev/null <<< "$raw"
import json,sys
try:
    arr=json.loads(sys.stdin.read())
    if isinstance(arr,list):
        for x in arr:
            if x is None: continue
            s=str(x).strip()
            if s: print(s)
except Exception:
    pass
PY
      return 0
    fi
  fi
  # Rule 2: comma split
  echo "$raw" \
  | tr -d '\r' \
  | sed 's/^[[:space:]]*//; s/[[:space:]]*$//' \
  | awk 'BEGIN{RS=","; ORS="\n"} {gsub(/^[[:space:]]+|[[:space:]]+$/, "", $0); if(length($0)) print $0}'
}

# Heuristic: second space-separated token looks like filename (one dot)
second_piece_looks_like_filename() {
  local raw="$*"
  local t1 t2
  read -r t1 t2 _ <<< "$raw"
  [[ -n "${t2:-}" ]] || return 1
  if [[ "$t2" =~ ^[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+$ ]]; then
    return 0
  fi
  return 1
}

# ---------------------------------------------------------------------
# Big per-file header (figlet + lolcat if present) + meta (size, path, tag cloud)
# ---------------------------------------------------------------------
print_file_header_banner() {
  local FILE="$1"
  local SIZE_KB="$2"
  local SIZE_BYTES
  SIZE_BYTES="$(filesize_bytes "$FILE" 2>/dev/null || echo 0)"
  local FULLPATH
  FULLPATH="$(fullpath_of "$FILE")"
  local BASENAME="${FILE##*/}"

  echo
  echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
  if have_cmd figlet; then
    if have_cmd lolcat; then
      figlet -w 120 "$BASENAME" | lolcat
    else
      figlet -w 120 "$BASENAME"
    fi
  else
    printf "=== %s ===\n" "$BASENAME"
  fi

  echo "ğŸ“„ Path: $FULLPATH"
  echo "ğŸ“ Size: ${SIZE_KB}KB (${SIZE_BYTES} bytes)"

  if ! is_binary_by_ext "$FILE" && is_text_file_fast "$FILE"; then
    echo "â˜ï¸  Tag Cloud (top ${PER_FILE_WORDS}):"
    # Cap tag-cloud scan to MAX_PRINT_BYTES for speed (matches print/analytics cap behavior)
    word_counts_file_capped "$FILE" "$MAX_PRINT_BYTES" | head -n "$PER_FILE_WORDS" | awk '{printf "  %-20s %s\n",$1,$2}'
  else
    echo "â˜ï¸  Tag Cloud: (binary-like; skipped)"
  fi

  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
}

# ---------------------------------------------------------------------
# Analytics + verification footer (parametrized, no /tmp leakage)
#   Args:
#     $1 -> path to TMP_ALL_TEXT (may be empty)
#     $2 -> array name of SELECTED files (for biggest files stats)
#     $3 -> related-mode ("filter" | "none")
#     $4 -> display command (pretty, e.g., "lsd a requirements.txt, README.md")
#     $5 -> planned count to display (or "â€”")
# ---------------------------------------------------------------------
print_analytics_and_verification() {
  local TMP_ALL_TEXT="$1"
  local -n SEL_ARR="$2"
  local RELATED_MODE="$3"
  local DISPLAY_CMD="$4"
  local DISPLAY_PLANNED="$5"

  echo -e "\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
  echo "ğŸ§© RELATED FILES & ANALYTICS"
  echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

  if [[ "$RELATED_MODE" == "filter" ]]; then
    # ALL_FILES is already built in filter mode; avoid re-scanning.
    $A_MODE && ensure_all_files_built

    local after=""; after="${DISPLAY_CMD#lsd a }"
    mapfile -t TOKS < <(parse_filter_tokens_from_args "$after")

    RELATED_CAND=()
    if ((${#TOKS[@]} > 0)); then
      for f in "${ALL_FILES[@]}"; do
        local skip=false
        for s in "${SEL_ARR[@]}"; do
          [[ "$f" == "$s" ]] && { skip=true; break; }
        done
        $skip && continue
        for t in "${TOKS[@]}"; do
          base="${t##*/}"
          [[ -z "$base" ]] && continue
          shopt -s nocasematch
          if [[ "$f" == *"$base"* ]]; then
            RELATED_CAND+=("$f")
            shopt -u nocasematch
            break
          fi
          shopt -u nocasematch
        done
      done
    fi

    if ((${#RELATED_CAND[@]} > 0)); then
      TMP_REL=$(mktemp)
      for r in "${RELATED_CAND[@]}"; do echo "$r"; done | awk '!seen[$0]++' | head -n $TOP_RELATED_FILES > "$TMP_REL"
      echo "Top $TOP_RELATED_FILES Related:"
      while IFS= read -r rf; do
        echo "â€¢ $rf"
        if is_binary_by_ext "$rf"; then
          echo " (binary-like; mini wordcloud skipped)"
        else
          word_counts_file_capped "$rf" "$MAX_PRINT_BYTES" | head -n $TOP_RELATED_WORDS | awk '{printf " %-20s %s\n",$1,$2}'
        fi
      done < "$TMP_REL"
      rm -f "$TMP_REL"
    else
      echo "(No related files found.)"
    fi
  else
    echo "(No related files â€” legacy full-dump has no filters.)"
  fi

  if [[ -s "$TMP_ALL_TEXT" ]]; then
    echo -e "\nğŸ“ˆ Top $TOP_WORDS_COUNT Words (selected files):"
    TMP_WORDS=$(mktemp)
    normalize_text < "$TMP_ALL_TEXT" | awk '{f[$0]++} END{for(w in f) print w, f[w]}' | LC_ALL=C sort -k2,2nr -k1,1 > "$TMP_WORDS"
    head -n $TOP_WORDS_COUNT "$TMP_WORDS" | awk '{printf " %-20s %s\n",$1,$2}'

    echo -e "\nğŸ§¬ Top $TOP_NGRAMS N-Gram Chains (score=count+10*len, up to len=5):"
    TMP_NG=$(mktemp)
    ngrams_scored < "$TMP_ALL_TEXT" > "$TMP_NG"
    head -n $TOP_NGRAMS "$TMP_NG" | awk '{printf " %-2s %-60s score=%s count=%s len=%s\n", NR".", $1, $2, $3, $4}'

    echo -e "\nğŸ‹ï¸ BIGGEST $TOP_BIGGEST_FILES FILES â€” TOP $TOP_BIGGEST_WORDS words"
    TMP_SIZES=$(mktemp)
    for f in "${SEL_ARR[@]}"; do
      [ -f "$f" ] || continue
      printf "%015d\t%s\n" "$(filesize_bytes "$f")" "$f" >> "$TMP_SIZES"
    done
    if [ -s "$TMP_SIZES" ]; then
      LC_ALL=C sort -r "$TMP_SIZES" | head -n $TOP_BIGGEST_FILES | cut -f2- | while read -r bf; do
        echo "â€¢ $bf"
        if is_binary_by_ext "$bf"; then
          echo " (binary-like; wordcloud skipped)"
        else
          word_counts_file_capped "$bf" "$MAX_PRINT_BYTES" | head -n $TOP_BIGGEST_WORDS | awk '{printf " %-20s %s\n",$1,$2}'
        fi
      done
    else
      echo "(No selected files to analyze for biggest files.)"
    fi
    rm -f "$TMP_WORDS" "$TMP_NG" "$TMP_SIZES"
  else
    echo -e "\n(No analyzable text from selected files.)"
  fi

  echo -e "\nğŸ§ª Command Verification:"
  echo " COMMAND: $DISPLAY_CMD"
  echo " Planned files: $DISPLAY_PLANNED"
  echo -e "\nğŸ”– [Q] Version 5.2"
  echo -e "\nğŸ§¿ Rarest / Longest Unique Token:"
  if [[ -s "$TMP_ALL_TEXT" ]]; then
    RAREST_LONGEST="$(longest_rarest_token < "$TMP_ALL_TEXT")"
  else
    RAREST_LONGEST=""
  fi
  if [[ -n "$RAREST_LONGEST" ]]; then
    echo " guess which longest rarest word of the day is: $RAREST_LONGEST"
  else
    echo " guess which longest rarest word of the day is: (none found)"
  fi
}

# ---------------------------------------------------------------------
# Always print collapsed tree first
# ---------------------------------------------------------------------
print_directory_tree_collapsed

# ---------------------------------------------------------------------
# MODE SWITCH
#   â€¢ LEGACY: first arg == "a" and $# == 1
#   â€¢ FILTER: first arg == "a" and $# >= 2
#   â€¢ Otherwise: end after tree.
# ---------------------------------------------------------------------
if [[ "$#" -gt 0 ]]; then
  SCOPE="$1"

  if [[ "$SCOPE" == "a" && "$#" -eq 1 ]]; then
    # =========================
    # LEGACY MODE: "lsd a"
    # =========================
    echo -e "\nğŸŒˆ Enhanced Dump Mode Triggered (LEGACY â€” full dump)\n"

    build_all_files_array
    FILE_COUNT="${#ALL_FILES[@]}"

    if have_cmd figlet && have_cmd lolcat; then
      echo "$FILE_COUNT FILES FOUND" | figlet | lolcat
    else
      echo -e "\nğŸ“¦ Total files found: $FILE_COUNT"
    fi

    echo -e "\nğŸ“š FILES IN ORDER (LEGACY FULL MODE):"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    TMP_ALL_TEXT="$(mktemp)"
    SELECTED=()

    CHUNK_I=0

    if [ "$FILE_COUNT" -eq 0 ]; then
      echo "ğŸš« No files found."
    else
      for FILE in "${ALL_FILES[@]}"; do
        FILE_SIZE_KB="$(filesize_kb "$FILE")"
        print_file_header_banner "$FILE" "$FILE_SIZE_KB"

        SELECTED+=("$FILE")

        if is_json_file "$FILE"; then
          # JSON: print NO contents, but still include in analytics buffer (capped) and show tag cloud above
          SIZE_BYTES=$(filesize_bytes "$FILE")
          echo "âš ï¸ JSON file â€” printing Tag Cloud only; contents suppressed."
          if [ "$SIZE_BYTES" -gt "$MAX_PRINT_BYTES" ]; then
            { echo; head -c "$MAX_PRINT_BYTES" "$FILE"; echo; } >> "$TMP_ALL_TEXT"
          else
            { echo; cat "$FILE"; echo; } >> "$TMP_ALL_TEXT"
          fi
        elif is_binary_by_ext "$FILE"; then
          echo "âš ï¸ Skipping image or binary file by extension."
        elif is_text_file_fast "$FILE"; then
          SIZE_BYTES=$(filesize_bytes "$FILE")
          if [ "$SIZE_BYTES" -gt "$MAX_PRINT_BYTES" ]; then
            echo "âš ï¸ Large file (>300KB). Displaying partial content."
            head -c "$MAX_PRINT_BYTES" "$FILE"
            { echo; head -c "$MAX_PRINT_BYTES" "$FILE"; echo; } >> "$TMP_ALL_TEXT"
          else
            echo "(Full file content below):"
            cat "$FILE"
            { echo; cat "$FILE"; echo; } >> "$TMP_ALL_TEXT"
          fi
        else
          echo "âš ï¸ Skipping binary or non-text file."
        fi

        echo -e "\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

        # Chunking: pause every CHUNK_SIZE files to prevent huge repo terminal hangs
        CHUNK_I=$((CHUNK_I + 1))
        if [[ "$CHUNK_SIZE" -gt 0 && "$CHUNK_SLEEP" -gt 0 && $((CHUNK_I % CHUNK_SIZE)) -eq 0 ]]; then
          echo -e "\nâ¸ï¸  Chunk pause: processed $CHUNK_I files. Sleeping ${CHUNK_SLEEP}s...\n"
          sleep "$CHUNK_SLEEP"
        fi
      done
    fi

    DISPLAY_CMD="lsd a"
    print_analytics_and_verification "$TMP_ALL_TEXT" SELECTED "none" "$DISPLAY_CMD" "â€”"
    rm -f "$TMP_ALL_TEXT"

    ACTUAL_COUNT="${#SELECTED[@]}"
    # Legacy has no planned logic; mirror total as "guessed" to satisfy the format without sharing logic
    echo -e "\nâœ… All file dumps complete. [$ACTUAL_COUNT files total | $ACTUAL_COUNT files guessed]\n"
    exit 0
  fi

  if [[ "$SCOPE" == "a" && "$#" -ge 2 ]]; then
    # =========================
    # FILTER MODE: "lsd a <filters...>"
    # =========================
    FILTER_RAW="${*:2}"   # everything after "a"

    build_all_files_array

    # Tokenize per rules (JSON first; else comma list)
    mapfile -t FILTER_TOKENS < <(parse_filter_tokens_from_args "$FILTER_RAW")

    # If user provided unquoted second arg (space-delimited) like: lsd a requirements.txt
    if [[ "${#FILTER_TOKENS[@]}" -eq 0 ]] && second_piece_looks_like_filename "$FILTER_RAW"; then
      read -r _ second _ <<< "$FILTER_RAW"
      FILTER_TOKENS+=("$second")
    fi

    # Build a canonical display string for the COMMAND line (no /tmp, no internal args)
    if ((${#FILTER_TOKENS[@]} > 0)); then
      DISPLAY_FILTERS="$(printf "%s\n" "${FILTER_TOKENS[@]}" | paste -sd', ' -)"
    else
      DISPLAY_FILTERS="$(echo "$FILTER_RAW" | sed 's/^[[:space:]]*//; s/[[:space:]]*$//')"
    fi
    DISPLAY_CMD="lsd a ${DISPLAY_FILTERS}"

    # Planned selection â€” broadened and normalized (counts filename-ish tokens regardless of existence)
    PLANNED_COUNT=0
    if ((${#FILTER_TOKENS[@]} > 0)); then
      for t in "${FILTER_TOKENS[@]}"; do
        if is_clean_filename "$t"; then
          PLANNED_COUNT=$((PLANNED_COUNT+1))
          continue
        fi

        nt="$(strip_leading_dotslash "$t")"
        if [[ -n "${PATH_NORM_SET["$nt"]:-}" || -n "${BASENAME_SET["$t"]:-}" ]]; then
          PLANNED_COUNT=$((PLANNED_COUNT+1))
        fi
      done
    fi

    echo -e "\nğŸ§­ Scope: \033[1m$SCOPE\033[0m ğŸ¯ Filters: \033[1m${DISPLAY_FILTERS:-<none>}\033[0m ğŸ“Š Planned selection: \033[1m$PLANNED_COUNT\033[0m"
    echo -e "\nğŸ“š FILES IN ORDER (FILTER MODE):"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    # Selection: exact PATH (normalized) plus ALL exact BASENAME matches (no substring fallback)
    SELECTED=()
    if ((${#FILTER_TOKENS[@]} > 0)); then
      # exact paths (normalized)
      for t in "${FILTER_TOKENS[@]}"; do
        nt="$(strip_leading_dotslash "$t")"
        if [[ -n "${NORMPATH_TO_FILE["$nt"]:-}" ]]; then
          SELECTED+=("${NORMPATH_TO_FILE["$nt"]}")
        fi
      done

      # ALL basenames that match
      for t in "${FILTER_TOKENS[@]}"; do
        base="${t##*/}"
        if [[ -n "${BASENAME_TO_FILES["$base"]:-}" ]]; then
          while IFS= read -r m; do
            [[ -n "$m" ]] && SELECTED+=("$m")
          done <<< "${BASENAME_TO_FILES["$base"]}"
        fi
      done

      # de-dup (preserve order) â€” optimized (no mktemp/awk)
      if ((${#SELECTED[@]} > 0)); then
        declare -A _seen_sel
        _seen_sel=()
        tmp_sel=()
        for s in "${SELECTED[@]}"; do
          [[ -n "${_seen_sel["$s"]:-}" ]] && continue
          _seen_sel["$s"]=1
          tmp_sel+=("$s")
        done
        SELECTED=( "${tmp_sel[@]}" )
      fi
    fi

    TMP_ALL_TEXT="$(mktemp)"
    CHUNK_I=0

    if ((${#SELECTED[@]} == 0)); then
      echo "ğŸš« Filter matched no files."
    else
      for FILE in "${SELECTED[@]}"; do
        FILE_SIZE_KB="$(filesize_kb "$FILE")"
        print_file_header_banner "$FILE" "$FILE_SIZE_KB"

        if is_json_file "$FILE"; then
          SIZE_BYTES=$(filesize_bytes "$FILE")
          echo "âš ï¸ JSON file â€” printing Tag Cloud only; contents suppressed."
          if [ "$SIZE_BYTES" -gt "$MAX_PRINT_BYTES" ]; then
            { echo; head -c "$MAX_PRINT_BYTES" "$FILE"; echo; } >> "$TMP_ALL_TEXT"
          else
            { echo; cat "$FILE"; echo; } >> "$TMP_ALL_TEXT"
          fi
        elif is_binary_by_ext "$FILE"; then
          echo "âš ï¸ Skipping image or binary file by extension."
        elif is_text_file_fast "$FILE"; then
          SIZE_BYTES=$(filesize_bytes "$FILE")
          if [ "$SIZE_BYTES" -gt "$MAX_PRINT_BYTES" ]; then
            echo "âš ï¸ Large file (>300KB). Displaying partial content."
            head -c "$MAX_PRINT_BYTES" "$FILE"
            { echo; head -c "$MAX_PRINT_BYTES" "$FILE"; echo; } >> "$TMP_ALL_TEXT"
          else
            echo "(Full file content below):"
            cat "$FILE"
            { echo; cat "$FILE"; echo; } >> "$TMP_ALL_TEXT"
          fi
        else
          echo "âš ï¸ Skipping binary or non-text file."
        fi

        echo -e "\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

        # Chunking: pause every CHUNK_SIZE files to prevent huge repo terminal hangs
        CHUNK_I=$((CHUNK_I + 1))
        if [[ "$CHUNK_SIZE" -gt 0 && "$CHUNK_SLEEP" -gt 0 && $((CHUNK_I % CHUNK_SIZE)) -eq 0 ]]; then
          echo -e "\nâ¸ï¸  Chunk pause: processed $CHUNK_I files. Sleeping ${CHUNK_SLEEP}s...\n"
          sleep "$CHUNK_SLEEP"
        fi
      done
    fi

    # Footer (filter mode) â€” NO /tmp in COMMAND
    print_analytics_and_verification "$TMP_ALL_TEXT" SELECTED "filter" "$DISPLAY_CMD" "$PLANNED_COUNT"
    rm -f "$TMP_ALL_TEXT"

    ACTUAL_COUNT="${#SELECTED[@]}"
    if [[ "$ACTUAL_COUNT" -eq "$PLANNED_COUNT" ]]; then
      echo -e "\nâœ… All file dumps complete. [$ACTUAL_COUNT files total | $PLANNED_COUNT files guessed]\n"
    else
      echo -e "\nğŸŸ¡ All file dumps complete. [$ACTUAL_COUNT files total | $PLANNED_COUNT files guessed]\n"
    fi
    exit 0
  fi

  # Any other scope: end after tree
  exit 0
else
  # No args: only collapsed tree shown.
  exit 0
fi
